[
  {
    "objectID": "pages/projects/wafflebot.html",
    "href": "pages/projects/wafflebot.html",
    "title": "Autonomous Maze Navigation with ROS2 and LiDAR",
    "section": "",
    "text": "This project implements autonomous maze navigation for a TurtleBot3 Waffle Pi robot using LiDAR-based perception and reactive control. The robot begins in an unknown orientation within a simply connected maze and must navigate to the exit and stop once it has fully exited. Sensing is provided by a 360° LiDAR scanner, and motion is achieved through differential-drive control.\nThe algorithm was initially testing virtually with Gazebo, and later tested on actual hardware.\nAt a high level, the robot evaluates obstacle distances in the forward, left, and right directions and selects motion commands accordingly. Navigation is based on wall-following behavior, which guarantees eventual maze exit under the assumption of continuous, unbroken walls.\n\n\n\nTesting Maze Algorithm in Gazebo"
  },
  {
    "objectID": "pages/projects/wafflebot.html#overview",
    "href": "pages/projects/wafflebot.html#overview",
    "title": "Autonomous Maze Navigation with ROS2 and LiDAR",
    "section": "",
    "text": "This project implements autonomous maze navigation for a TurtleBot3 Waffle Pi robot using LiDAR-based perception and reactive control. The robot begins in an unknown orientation within a simply connected maze and must navigate to the exit and stop once it has fully exited. Sensing is provided by a 360° LiDAR scanner, and motion is achieved through differential-drive control.\nThe algorithm was initially testing virtually with Gazebo, and later tested on actual hardware.\nAt a high level, the robot evaluates obstacle distances in the forward, left, and right directions and selects motion commands accordingly. Navigation is based on wall-following behavior, which guarantees eventual maze exit under the assumption of continuous, unbroken walls.\n\n\n\nTesting Maze Algorithm in Gazebo"
  },
  {
    "objectID": "pages/projects/wafflebot.html#simulation-and-control-logic",
    "href": "pages/projects/wafflebot.html#simulation-and-control-logic",
    "title": "Autonomous Maze Navigation with ROS2 and LiDAR",
    "section": "Simulation and Control Logic",
    "text": "Simulation and Control Logic\nDevelopment and validation were first performed in Gazebo to ensure correct behavior under ideal sensing conditions before deployment to hardware. The final implementation subscribes to LiDAR data from the /scan topic and publishes twist messages to /cmd_vel at 10 Hz.\n\n\nShow Maze Solving code\n\n\n# Copyright 2016 Open Source Robotics Foundation, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport rclpy\nfrom rclpy.node import Node\nimport math\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import LaserScan\n\n\nclass MazeSolver(Node):\n\n    # Robot direction corresponding to the lidar index/angle. Specific to gazebo sim\n    FRONT = 360\n    BACK = 0\n    RIGHT = 180\n    LEFT = 540\n\n    # General tolerance value\n    TOL = 0.4\n\n    # linear and angular velocities\n    LINEAR_VEL = 0.5\n    ROT_VEL = 0.5\n\n    # The angle deviating from the left and right angles, used in maintain parallel\n    # LEFT+RAY_ANGLE, LEFT-RAY_ANGLE, RIGHT+RAY_ANGLE, and RIGHT_ANGLE \n    RAY_ANGLE = 20\n\n    def __init__(self):\n        super().__init__('maze_solver')\n\n        # Create publisher and subscriber \n        self.cmd_publisher = self.create_publisher(Twist, '/cmd_vel', 10)\n        self.scan_subscription = self.create_subscription(LaserScan,'/scan',\n                                                          self.scan_callback,\n                                                          10)\n        # Timer for calling move\n        self.timer = self.create_timer(0.1, self.move)\n        \n        # Initialize directional distances\n        self.front = 0.0\n        self.right = self.front_right = self.back_right = 0.0\n        self.left = self.front_left = self.back_left = 0.0\n\n        # Minimum and maximum valid sitance values\n        self.range_min = 1.0\n        self.range_max = 100.0\n\n        # Boolean flags for turning\n        self.is_moving = self.is_turning_left = self.is_turning_right = False\n\n        # Counter for counting cycles, default None (not initialized)\n        self.counter = None\n\n        # Declare and get parameter\n        self.declare_parameter('direction', 1)\n        self.declare_parameter('distance', 0.5)        \n        self.direction = self.get_parameter('direction').get_parameter_value().integer_value\n        self.distance = self.get_parameter('distance').get_parameter_value().double_value\n\n        # Calculate the cosine of the ray angle first to use it later in the\n        # right triangle calculations.\n        self.__trig = math.cos(MazeSolver.RAY_ANGLE / 180 * math.pi)\n\n    def scan_callback(self, msg):\n        \"\"\"\n        ROBOT:\n        right = 180\n        back = 0\n        front = 360\n        left = 540\n\n        Update all directional distances from the message sent by /scan\n        \"\"\"\n\n        self.front = min(msg.ranges[MazeSolver.FRONT-40 : MazeSolver.FRONT+40])\n\n        self.right = min(msg.ranges[MazeSolver.RIGHT-10 : MazeSolver.RIGHT+10])\n        self.front_right = min(msg.ranges[MazeSolver.RIGHT + MazeSolver.RAY_ANGLE - 5 : MazeSolver.RIGHT + MazeSolver.RAY_ANGLE + 5 ])\n        self.back_right =  min(msg.ranges[MazeSolver.RIGHT - MazeSolver.RAY_ANGLE - 5 : MazeSolver.RIGHT - MazeSolver.RAY_ANGLE + 5 ])\n\n        self.left = min(msg.ranges[MazeSolver.LEFT-10 : MazeSolver.LEFT+10])\n        self.front_left = min(msg.ranges[MazeSolver.LEFT - MazeSolver.RAY_ANGLE - 5 : MazeSolver.LEFT - MazeSolver.RAY_ANGLE + 5 ])\n        self.back_left =  min(msg.ranges[MazeSolver.LEFT + MazeSolver.RAY_ANGLE - 5 : MazeSolver.LEFT + MazeSolver.RAY_ANGLE + 5 ])\n\n        self.range_min = msg.range_min\n        self.range_max = msg.range_max\n\n    def turn_left(self) -&gt; Twist:\n        # Basic turn left function sets CCW angular velocity and 0.0 linear velocity\n        command = Twist()\n        command.angular.z = MazeSolver.ROT_VEL\n        command.linear.x = 0.0\n\n        return command\n\n    def turn_right(self) -&gt; Twist:\n        # Basic turn right function sets CW angular velocity and 0.0 linear velocity\n        command = Twist()\n        command.angular.z = -MazeSolver.ROT_VEL\n        command.linear.x = 0.0\n\n        return command\n\n    def wall_follow(self) -&gt; Twist:\n        command = Twist()\n        hardware = 5\n\n        # Obtain angular velocity by calculating the difference in distnace from\n        # the fornt and back rays.\n        right_vel = self.back_right - self.front_right\n        left_vel = self.front_left - self.back_left\n\n        # Bound the calculated velocity to have magnitude less then MazeSolver.ROT_VEL\n        right_vel = (-MazeSolver.ROT_VEL if right_vel &lt; 0 else MazeSolver.ROT_VEL) if abs(right_vel) &gt; MazeSolver.ROT_VEL else right_vel\n        left_vel = (-MazeSolver.ROT_VEL if left_vel &lt; 0 else MazeSolver.ROT_VEL) if abs(left_vel) &gt; MazeSolver.ROT_VEL else left_vel\n\n        command.linear.x = MazeSolver.LINEAR_VEL    # Set linear speed to global constant\n        \n        # Check if the calculated values are NaN if calculated velocity on one side is NaN\n        # then use velocity on the other. If neither side is NaN (usual case) then set Twist\n        # angular velocity to be the one that is smaller in magnitude.\n        if not math.isnan(right_vel) and math.isnan(left_vel):\n            self.get_logger().info(\"right !nan, left nan\")\n            command.angular.z = right_vel * hardware  \n        elif not math.isnan(left_vel) and math.isnan(right_vel):\n            self.get_logger().info(\"left !nan, right nan\")\n            command.angular.z = left_vel * hardware \n        elif not math.isnan(right_vel) and not math.isnan(left_vel):\n            self.get_logger().info(\"right !nan, left !nan\")\n            command.angular.z = min(right_vel, left_vel, key=abs) * hardware \n\n        self.get_logger().info(f\"vel: {command.angular.z}\")\n\n        return command\n    \n    def finished_turning(self) -&gt; bool:\n        # Use trig and the front back distance rays to calculate right and left  \n        # distances.\n        calc_right_front = self.front_right * self.__trig\n        calc_right_back = self.back_right * self.__trig\n\n        calc_left_front = self.front_left * self.__trig\n        calc_left_back = self.back_left * self.__trig\n\n        # Compare the calculated values with the actual values and compare front\n        # and back distnace values. Since lidar will never actuall give equal\n        # distances, as long as the difference is less then TOL/2 (0.05) then we stop\n        is_finished = (abs(self.front_right - self.back_right) &lt; MazeSolver.TOL/2 and\n                       abs(calc_right_back - self.right) &lt; MazeSolver.TOL/2 and\n                       abs(calc_right_front - self.right) &lt; MazeSolver.TOL/2) or \\\n                      (abs(self.front_left - self.back_left) &lt; MazeSolver.TOL/2 and\n                       abs(calc_left_back - self.left) &lt; MazeSolver.TOL/2 and\n                       abs(calc_left_front - self.left) &lt; MazeSolver.TOL/2)\n        \n        if is_finished:\n            self.get_logger().info(f\"Finished turning\")\n\n        return is_finished\n \n    def move(self):\n        # Check if counter is initialized\n        if self.counter != None:\n            self.counter += 1\n\n            # If counter is greater then 10 start checking for whether we have reached\n            # a parallel position and needs to stop turning.\n            if self.counter &gt; 10:\n                if self.is_turning_right and self.finished_turning():\n                    self.get_logger().info(\"Turning right finished\")\n                    self.is_turning_right = False\n                    self.counter = None\n                elif self.is_turning_left and self.finished_turning():\n                    self.get_logger().info(\"Turning left finished\")\n                    self.is_turning_left = False\n                    self.counter = None\n            \n        # If front distance is less then stopping distance or currently turning\n        if self.is_turning_left or self.is_turning_right or \\\n        (self.front &gt; self.range_min and self.front &lt; self.distance):\n            \n            # Turn right if currently turning right or right distance greater then left\n            if not self.is_turning_left and (self.is_turning_right or self.right &gt; self.left):\n                command = self.turn_right()\n                self.get_logger().info(\"START turn right\")\n\n                self.is_turning_right = True\n                if self.counter == None:\n                    self.counter = 0\n            elif not self.is_turning_right and (self.is_turning_left or self.left &gt; self.right):\n                # Turn left if currently turning left or left distance greater then right\n                command = self.turn_left()\n                self.get_logger().info(\"START turn left\")\n\n                self.is_turning_left = True\n                if self.counter == None:\n                    self.counter = 0\n        else:\n            command = self.wall_follow()\n\n        self.cmd_publisher.publish(command)\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    # Create object and spin\n    maze_solver = MazeSolver()\n    rclpy.spin(maze_solver)\n\n    # Destory node explicitly\n    maze_solver.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n\nEight LiDAR rays are extracted to represent front, back, left, right, and supporting front/back measurements on each side. These values are used for obstacle detection, wall following, turn completion, and exit detection. Motion logic follows a simple priority structure:\n\nStop rotation if parallel alignment with a wall is detected\nInitiate rotation if an obstacle is detected within a forward stopping distance\nOtherwise, proceed forward using wall-following control\n\nRotation is triggered when a frontal obstacle is detected and continues for a fixed number of cycles before checking for alignment with nearby walls. Parallel alignment is determined using geometric consistency checks between LiDAR ray distances and trigonometric projections. Multiple checks are used to improve robustness against noisy measurements.\n\n\n\nSlope of wall is approximated by the distance from three rays. Correction is applied to maintain parallel movement to wall\n\n\nWall following is achieved by comparing front and back LiDAR rays on each side. The angular velocity command is proportional to the difference between these measurements, with larger differences producing stronger corrective turns. When operating in confined spaces, angular corrections are scaled to improve stability. The final angular velocity is chosen as the smaller-magnitude correction between the left and right sides to minimize oscillations.\nExit detection is handled through a state-based procedure. Upon detecting open space consistent with leaving the maze, the robot moves forward slightly and performs a 180° rotation. Exit confirmation occurs when LiDAR measurements behind and to both sides return infinite distances."
  },
  {
    "objectID": "pages/projects/wafflebot.html#hardware-deployment",
    "href": "pages/projects/wafflebot.html#hardware-deployment",
    "title": "Autonomous Maze Navigation with ROS2 and LiDAR",
    "section": "Hardware Deployment",
    "text": "Hardware Deployment\nThe algorithm was deployed on a TurtleBot3 Waffle Pi platform, with the robot and control laptop connected over a shared network. LiDAR data is streamed from the robot, and velocity commands are issued remotely.\nSeveral physical considerations affect hardware behavior. LiDAR index mappings differ between simulation and hardware and must be adjusted accordingly. Additionally, the robot rotates about the wheel axis rather than its geometric center, requiring sufficient clearance to avoid wall contact during turns. Concurrent teleoperation was disabled to prevent conflicting velocity commands.\n\n\n\nLab test of Physical Robot and Maze"
  },
  {
    "objectID": "pages/projects/wafflebot.html#performance-and-limitations",
    "href": "pages/projects/wafflebot.html#performance-and-limitations",
    "title": "Autonomous Maze Navigation with ROS2 and LiDAR",
    "section": "Performance and Limitations",
    "text": "Performance and Limitations\nIn simulation, the robot consistently exited all provided maze configurations without wall collisions. On hardware, reliable wall following and obstacle avoidance were observed in environments with wide corridors and clear boundaries.\nFailure cases occurred when obstacles were positioned diagonally or when narrow walls were encountered head-on. In these scenarios, limited frontal sensing caused missed detections, or competing corrections resulted in oscillatory rotation behavior. These issues highlight limitations in the obstacle detection cone and turn-selection logic.\nEarlier emphasis on a strict right-hand-rule implementation and expanded diagonal obstacle detection would improve robustness and guarantee maze completion in all simply connected environments."
  },
  {
    "objectID": "pages/projects/lego_pass.html",
    "href": "pages/projects/lego_pass.html",
    "title": "Collaborative Lego Passing Robots",
    "section": "",
    "text": "In our Robotics Perception and Planning class, the goal was to program two robots to autonomously pass Legos between each other while navigating around obstacles. Robot 1 was responsible for picking up a Lego, moving to a designated blue line, and coordinating a handoff with Robot 2. Robot 2 would then transport the Lego to the goal.\nBoth robots relied on object detection, line detection, AprilTag-based localization, and path planning to perceive their environment, detect obstacles, Legos, and other robots, and communicate reliably using TCP and ZeroMQ to ensure synchronized handoffs.\n\n\n\nFinal Project gif, Courtesy of UMD"
  },
  {
    "objectID": "pages/projects/lego_pass.html#overview",
    "href": "pages/projects/lego_pass.html#overview",
    "title": "Collaborative Lego Passing Robots",
    "section": "",
    "text": "In our Robotics Perception and Planning class, the goal was to program two robots to autonomously pass Legos between each other while navigating around obstacles. Robot 1 was responsible for picking up a Lego, moving to a designated blue line, and coordinating a handoff with Robot 2. Robot 2 would then transport the Lego to the goal.\nBoth robots relied on object detection, line detection, AprilTag-based localization, and path planning to perceive their environment, detect obstacles, Legos, and other robots, and communicate reliably using TCP and ZeroMQ to ensure synchronized handoffs.\n\n\n\nFinal Project gif, Courtesy of UMD"
  },
  {
    "objectID": "pages/projects/lego_pass.html#architecture",
    "href": "pages/projects/lego_pass.html#architecture",
    "title": "Collaborative Lego Passing Robots",
    "section": "Architecture",
    "text": "Architecture\n\nLocalization and Path Planning (Lab 1):\n\nThe robot uses a pre-mapped environment with AprilTags as landmarks.\nShortest-path algorithms such as UCS and Dijkstra guide the robot toward checkpoints, avoiding obstacles by generating a graph where edges are weighted according to obstacle presence.\nDrive commands are executed via drive_speed() for precise movement while continuously verifying location using camera-based tag detection.\n\nLego Detection and Pickup (Lab 2):\n\nA trained YOLOv8 model detects Legos and other robots.\nBounding boxes provide the position and orientation data required for arm manipulation.\nOpenCV isolates the blue line, enabling Robot 1 to align correctly and position the Lego for handoff.\n\nObstacle Detection and Visual Odometry (Final Project):\n\nInfrared and camera-based sensors detect obstacles, including random U-shaped barriers near Legos.\nVisual odometry tracks robot motion relative to the environment, allowing dynamic trajectory adjustments.\nRobots retrace or reroute paths as needed to navigate safely.\n\n\n\n\n\nBlock Diagram\n\n\nStarting from a defined start position, Robot 1 locates Legos, picks them up, and uses infrared sensors to avoid obstacles while navigating to the blue line. Once aligned, Lab 2 code detects the line’s position and orientation to deposit the Lego accurately. The robot then retraces its path to repeat the process.\nRobot 2 follows a similar routine: avoiding obstacles, detecting the blue line, identifying Legos, picking them up, and planning a path to deliver them to the goal. This process repeats as more Legos become available.\nExpanding the training dataset to identify labeled boxes was considered but ultimately impractical due to time constraints, although it could have improved detection accuracy.\n\n\nShow code"
  },
  {
    "objectID": "pages/projects/lego_pass.html#key-functions",
    "href": "pages/projects/lego_pass.html#key-functions",
    "title": "Collaborative Lego Passing Robots",
    "section": "Key Functions",
    "text": "Key Functions\n\nApriltag Identification\nAprilTags enable precise localization by allowing the robot to determine its position and orientation in the environment.\n\nAprilTag Detector Setup – Initializes the detector for the tag36h11 family to identify tags in camera frames.\nRobot Initialization – Connects to the robot, starts the camera stream, and prepares the chassis for movement.\ngetRelativePos() – Continuously captures camera frames, converts them to grayscale, detects AprilTags, computes relative position and rotation using a PnP method, and returns transformed coordinates. Visual feedback is drawn on images for debugging.\nfind_pose_from_tag(K, detection) – Computes the 3D position and orientation of a detected tag using its known size and camera intrinsics.\n\n\n\nShow code\n\n    from pupil_apriltags import Detector\n    import cv2\n    import numpy as np\n    import time\n    from robomaster import robot\n    from robomaster import camera\n\n    currentPosition = [0,0] #x,y\n    currentAngle = 0\n\n    at_detector = Detector(\n        families=\"tag36h11\",\n        nthreads=1,\n        quad_decimate=1.0,\n        quad_sigma=0.0,\n        refine_edges=1,\n        decode_sharpening=0.25,\n        debug=0\n    )\n\n    #gives relative position (x,y) and angle to tag. requires find_pos_from_tag\n    def getRelativePos():\n\n        while True:\n            try:\n                img = ep_camera.read_cv2_image(strategy=\"newest\", timeout=0.5)  \n                cv2.imwrite(\"/home/user/Desktop/test.png\", img) \n                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n                gray.astype(np.uint8)\n\n                K=np.array([[184.752, 0, 320], [0, 184.752, 180], [0, 0, 1]])\n\n                results = at_detector.detect(gray, estimate_tag_pose=False)\n                for res in results:\n                    print(res)\n                    pose = find_pose_from_tag(K, res)\n                    rot, jaco = cv2.Rodrigues(pose[1], pose[1])\n                    print(rot)\n\n                    pts = res.corners.reshape((-1, 1, 2)).astype(np.int32)\n                    img = cv2.polylines(img, [pts], isClosed=True, color=(0, 0, 255), thickness=5)\n                    cv2.circle(img, tuple(res.center.astype(np.int32)), 5, (0, 0, 255), -1)\n\n                    print(pose)\n\n                    # (pose[0][0]) # y\n                    # (pose[0][2]) # x\n                \n                    curr_y = round(pose[0][0],2)\n                    curr_x = round(pose[0][2], 2)\n                    rot = round(pose[1][2], 2) * 180 / 3.14\n                    \n                    posi_initial = [[curr_x], [curr_y]]\n                    rotation = [[np.cos(rot),np.sin(rot)],[np.sin(-1*rot),np.cos(rot)]]\n                    posi_final = rotation*posi_initial\n                    \n                    return posi_final\n                \n                cv2.imshow(\"img\", img)\n                cv2.waitKey(1)\n\n            except KeyboardInterrupt:\n                ep_camera.stop_video_stream()\n                ep_robot.close()\n                print ('Exiting')\n                exit(1)\n\n    def find_pose_from_tag(K, detection):\n        tag_size=0.16 # tag size in meters\n        m_half_size = tag_size / 2\n\n        marker_center = np.array((0, 0, 0))\n        marker_points = []\n        marker_points.append(marker_center + (-m_half_size, m_half_size, 0))\n        marker_points.append(marker_center + ( m_half_size, m_half_size, 0))\n        marker_points.append(marker_center + ( m_half_size, -m_half_size, 0))\n        marker_points.append(marker_center + (-m_half_size, -m_half_size, 0))\n        _marker_points = np.array(marker_points)\n\n        object_points = _marker_points\n        image_points = detection.corners\n\n        pnp_ret = cv2.solvePnP(object_points, image_points, K, distCoeffs=None,flags=cv2.SOLVEPNP_IPPE_SQUARE)\n        if pnp_ret[0] == False:\n            raise Exception('Error solving PnP')\n\n        r = pnp_ret[1]\n        p = pnp_ret[2]\n\n        return p.reshape((3,)), r.reshape((3,))\n\n\n    if __name__ == '__main__':\n        ep_robot = robot.Robot()\n        ep_robot.initialize(conn_type=\"ap\")\n        ep_camera = ep_robot.camera\n        ep_camera.start_video_stream(display=False, resolution=camera.STREAM_360P)\n\n        ep_chassis = ep_robot.chassis\n\n\n\nShortest Path (Obstacle Navigation)\n\n\n\nNavigation Block Diagram\n\n\nPath planning allows the robot to navigate the optimal path given a map. For testing, a 10×13 grid is used, with walls represented by high-weight cells and free space by low-weight cells.\n\nucs(graph, start, goal) – Finds the shortest path using Uniform Cost Search, avoiding high-cost obstacles.\ngenerateNodeNeighbors(row, col, arrMap) – Generates valid neighboring nodes for each cell.\ncompute_path() – Builds a graph from the map, runs UCS to compute the optimal path from the start to the goal, and returns the sequence of nodes representing that path.\nvisualize_path(coordinates) – Plots the computed path on a grid to visualize the planned trajectory.\n\n\n\n\nShortest Path using UCS Algorithm\n\n\n\n\n\nvisualize_path(coordinates) output\n\n\n\n\nShow code\n\n    import queue\n    import matplotlib.pyplot as plt\n\n    def ucs(graph, start, goal):\n        visited = set()\n        frontier = queue.PriorityQueue()  # Using a priority queue\n        frontier.put((0, start, [start]))  # Adding start node with cost 0\n\n        while not frontier.empty():\n            cost, node, path = frontier.get()\n            if node not in visited:\n                visited.add(node)\n\n                if node == goal:\n                    return path  # Return the path to the goal\n\n                # Explore neighbors\n                for neighbor, neighbor_cost in graph[node].items():\n                    if neighbor not in visited:\n                        new_cost = cost + neighbor_cost\n                        frontier.put((new_cost, neighbor, path + [neighbor]))  # Add to the priority queue\n\n        return None  # Goal not found\n\n\n    #start new code\n    W = 256 # wall weight\n    #13x13 matrix \n    numRows = 10\n    numCols = 13 \n\n    Map =   [    [  W,  1,  1,  W,  W,  W,  W,  W,  W,  W,  1,  1,  W],\n                [  W,  1,  1,  W,  1,  1,  1,  1,  1,  W,  1,  1,  W],\n                [  W,  1,  1,  W,  1,  1,  1,  1,  1,  W,  1,  1,  W],\n                [  W,  1,  1,  W,  1,  1,  1,  1,  1,  W,  1,  1,  W],\n                [  W,  1,  1,  W,  1,  1,  W,  1,  1,  W,  1,  1,  W],\n                [  W,  1,  1,  1,  1,  1,  W,  1,  1,  1,  1,  1,  W],\n                [  W,  1,  1,  1,  1,  1,  W,  1,  1,  1,  1,  1,  W],\n                [  W,  1,  1,  1,  1,  1,  W,  1,  1,  1,  1,  1,  W],\n                [  W,  1,  1,  1,  1,  1,  W,  1,  1,  1,  1,  1,  W],\n                [  W,  W,  W,  W,  W,  W,  W,  W,  W,  W,  W,  W,  W],\n                ]\n\n    def generateNodeNeighbors(row, col, arrMap):\n        neighbors = {}\n        directions = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # Cardinal directions: right, left, down, up\n        for dir_row, dir_col in directions:\n            iRow = row + dir_row\n            jCol = col + dir_col\n            if 0 &lt;= iRow &lt; numRows and 0 &lt;= jCol &lt; numCols:  # Check if within map bounds\n                neighborName = str(iRow) + \" \" + str(jCol)\n                neighbors[neighborName] = arrMap[iRow][jCol]\n        return neighbors\n\n    # takes in boolean \n    def compute_path():\n        graphMap = {}\n        for i in range(numRows):\n            for j in range(numCols):\n                nodeName = str(i) + \" \" + str(j)\n                graphMap.update({nodeName:generateNodeNeighbors(i,j,Map)})\n\n        start_node = \"3 1\"\n        goal_node = \"3 11\"\n        path_to_goal = ucs(graphMap, start_node, goal_node)\n        if path_to_goal:\n            return path_to_goal\n        else:\n            return None\n\n    def visualize_path(coordinates):\n        x_coords = []\n        y_coords = []\n        for coord in coordinates:\n            x, y = coord.split()\n            x_coords.append(int(x))\n            y_coords.append(int(y))\n        \n        # Set up the grid\n        plt.figure()\n        plt.grid(True)\n            \n        # Plot the path\n        plt.plot(x_coords, y_coords, marker='o')\n        \n        # Set plot limits\n        plt.xlim(min(x_coords) - 1, max(x_coords) + 1)\n        plt.ylim(min(y_coords) - 1, max(y_coords) + 1)\n        \n        # Switch axis labels\n        plt.xlabel('Y')\n        plt.ylabel('X')\n        plt.title('Path')\n        # Show plot\n        plt.show()\n\n    path = compute_path()\n    visualize_path(path)\n\n\n\nLine Detection\n\nfind_blue(image) – Detects the blue line in the robot’s camera image, which serves as a reference for positioning or handoff. The function crops a region near the bottom of the image, converts it to HSV, thresholds for blue, finds contours, and fits a line. It returns the line’s slope, vertical position, and length.\n\n\n\n\nLine Detection\n\n\n\n\nShow code\n\ndef find_blue(image):\n    original_image = image.copy()  # preserve original img\n    img = np.array(image)\n    \n    # blue color range\n    lower_blue = np.array([70,68,90])\n    upper_blue = np.array([200,255,255])\n    cc = 0 # snip bounds\n    aa = 100\n    snip = np.zeros((img.shape[0], img.shape[1]), dtype =\"uint8\")\n    snip = img[(img.shape[0] - aa):(img.shape[0] - cc), (0):(img.shape[1])]\n\n    hsv = cv2.cvtColor(snip, cv2.COLOR_BGR2HSV)\n    mask = cv2.inRange(hsv, lower_blue, upper_blue)\n\n    # image contours\n    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    \n    if contours:\n        longest_contour = None\n        max_contour_length = 0\n    \n        # find longest contour\n        longest_contour = contours[0]\n        for contour in contours:\n            contour_length = cv2.arcLength(contour, True)\n            \n            if contour_length &gt; max_contour_length:\n                max_contour_length = contour_length\n                longest_contour = contour\n\n        vx, vy, x, y = cv2.fitLine(longest_contour, cv2.DIST_L2, 0, 0.01, 0.01)\n        contour_length = cv2.arcLength(longest_contour, True)\n        # determine line slope\n        slope = vy / vx\n        \n        # draw line\n        left_point = (int(x - 1000 * vx), int(y - 1000 * vy))\n        right_point = (int(x + 1000 * vx), int(y + 1000 * vy))\n        line_image = cv2.line(snip, left_point, right_point, (0, 255, 0), 2)\n        \n        #cv2.imshow(\"Mask\",mask)\n        cv2.imshow(\"Line\",line_image)\n        cv2.waitKey(10)\n        \n        # return angle, line position\n        return slope[0], y[0], contour_length\n    \n    else:\n        print(\"No blue line found in the image.\")\n        return 0, 0, 0\n\n\n\nEnvironment Perception and Object Identification\nThe YOLOv8 model was trained with 100 images collected via Roboflow, annotated with bounding boxes, and split into 70% training, 20% validation, and 10% test sets.\n\n\n\nDrawing Bounding Boxes\n\n\n\n\n\nEvaluating Model Accuracy\n\n\nThe robot initializes all components, including chassis, gripper, and robotic arm, and loads the YOLO model for object detection.\n\n\nShow code\n\nif __name__ == '__main__':\n    ep_robot = robot.Robot()\n    ep_robot.initialize(conn_type=\"ap\")\n    ep_camera = ep_robot.camera\n    ep_camera.start_video_stream(display=False, resolution=camera.STREAM_360P)\n    ep_chassis = ep_robot.chassis\n    ep_gripper = ep_robot.gripper\n    ep_arm = ep_robot.robotic_arm\n\n    model = YOLO('&lt;Filepath of Trained Model&gt;')\n\n\nfindLego(model) – Rotates the robot until a Lego is detected, then stops once the target is located.\ndriveToLego(model) – Fine-tunes robot position and orientation, calculating offsets and distance to the Lego using the bounding box. Moves iteratively until centered and at the correct distance for pickup.\n\n\n\nShow code\n\ndef findLego(model):\n    # spin until lego found\n    legoFound = False\n    z_val = 40 # degrees to spin\n    while not legoFound:\n        frame = ep_camera.read_cv2_image(strategy=\"newest\", timeout=5)  \n        cv2.imshow(\"Frame\",frame)\n        cv2.waitKey(10)\n        results = model.predict(source=frame, conf=0.8)\n\n        # stop spinning once lego is found\n        if len(results[0]) &gt; 0:\n            legoFound = True\n        else:\n            ep_chassis.drive_speed(x=0, y=0, z=z_val, timeout=5)\n            \n    # Stop spinning\n    ep_chassis.drive_speed(x=0, y=0, z=0, timeout=5)\n\ndef driveToLego(model):\n    \n    dist = []\n    target_distance = 1.6 # arbitrary\n    distance_tol = 0.07\n    box_offset_tol = 3 # num pixels left/right of center\n    ON_TARGET = False\n    while(not ON_TARGET): \n        frame = ep_camera.read_cv2_image(strategy=\"newest\", timeout=5)  \n        img = np.array(frame)\n        cv2.imshow(\"Frame\",frame)\n        cv2.waitKey(10)\n        results = model.predict(source=frame, conf=0.7)\n\n        conf = results[0].boxes.conf\n        conf = conf.cpu()\n        if len(conf) &gt; 0:\n            best = np.argmax(conf)\n        else:\n            continue\n        coords = results[0].boxes.xywh\n        \n        # find coords for bounding box w/ highest confidence\n        c = coords[best]\n        x_center = int(img.shape[1]/2)\n        x = int(c[0])\n        y = int(c[1])\n        w = int(c[2])\n        h = int(c[3])\n        box_offset = x_center - x\n        distance = box_distance(dist, w, h)\n        if (abs(box_offset) &gt; box_offset_tol) and not ON_TARGET:\n            # turn till box is in the center\n            Kz = -0.15\n            z_val = box_offset * Kz\n            ep_chassis.drive_speed(x=0, y=0, z=z_val, timeout=5)\n        elif (abs(distance - target_distance) &gt;= distance_tol) and not ON_TARGET :\n            # move till box is at target distance\n            Kx = 0.175\n            x_val = (distance - target_distance) * Kx\n            # print(distance)\n            ep_chassis.drive_speed(x=x_val, y=0, z=0, timeout=5)\n        elif (abs(distance - target_distance) &lt; distance_tol):\n            # done movement\n            ON_TARGET = True\n            x_val = 0.21 # move slightly further - adjust as needed\n            ep_chassis.drive_speed(x=0, y=0, z=0, timeout=5)\n            ep_chassis.move(x=x_val, y=0, z=0, xy_speed=0.7).wait_for_completed()\n\n    cv2.destroyAllWindows()\n\n\n\nAutonomous Robot Communication\nTo coordinate handoff, the following structure is used with ZeroMQ:\n\nRobot 1 (REQ) -&gt; sends “Reached line” -&gt; Robot 2 (REP)\nRobot 2 (REP) -&gt; replies “Release” -&gt; Robot 1 (REQ)\nRobot 1 -&gt; releases Lego -&gt; Robot 2\n\n\nSocket Setup\nRobot 1 and 2 initiates communication by creating a request socket and connecting to the server laptop hosting the reply socket.\n\n\nShow code\n\n\n    import zmq\n\n    context = zmq.Context()\n\n    # create socket\n    # server laptop ip addr\n    SERVER_ADDR = \"\"\n    print(\"Connecting to communication server…\")\n    socket = context.socket(zmq.REQ)\n    socket.connect(SERVER_ADDR)\n\n\n\nRobot 1: Passing robot\nRobot 1 is responsible for delivering the Lego to the blue line and releasing it once Robot 2 confirms it has arrived. After reaching the handoff location, Robot 1 sends a synchronization message and blocks until a reply is received. Only then does it release the Lego.\n\n\nShow code\n\n    # release lego once other robot has arrived\n    socket.send(b\"Reached line\")\n\n    #  wait on reply\n    message = socket.recv()\n    print(\"Received reply [ %s ]\" %  message)\n    # release after short wait\n    time.sleep(3)\n    ep_gripper.open(power=50)\n    time.sleep(1)\n    ep_gripper.pause()\n    ep_robot.close()\n    break\n\n\n\nRobot 2: Receiving Robot\nRobot 2 waits for Robot 1’s signal before attempting to retrieve the Lego. Upon receiving the message, it marks the handoff state as active and begins searching for the Lego to complete the transfer.\n\n\nShow code\n\n    message = socket.recv()\n    print(\"Received request: %s\" % message)\n    startMsgReceived = True\n\nOnce the synchronization message has been received, Robot 2 proceeds with detection and pickup logic.\n\n\nShow code\n\n    # search either for lego or robot\n    if name == 'lego' and startMsgReceived:\n        print(\"x: {}, y: {}\".format(x, y))\n        print(\"Width of Box: {}, Height of Box: {}\".format(w, h))\n        print(\"Distance: \" + str(distance_from_box_size(w, h)))\n        print(\"Detected object: \", name)\n\n        ## rest of code executes handoff"
  },
  {
    "objectID": "pages/projects/lego_pass.html#results",
    "href": "pages/projects/lego_pass.html#results",
    "title": "Collaborative Lego Passing Robots",
    "section": "Results",
    "text": "Results\nThe project presented challenges due to obstacles and dynamic conditions. Infrared sensors enabled navigation, and integration of previous Lab 2 functionality with new path-planning and obstacle-avoidance logic allowed consistent performance. Legos were successfully passed asynchronously across the field."
  },
  {
    "objectID": "pages/projects/lego_pass.html#lego-grab-and-handoff-sequence",
    "href": "pages/projects/lego_pass.html#lego-grab-and-handoff-sequence",
    "title": "Collaborative Lego Passing Robots",
    "section": "Lego Grab and Handoff Sequence",
    "text": "Lego Grab and Handoff Sequence\n\n\n\n\nStart\n\n\n\n\n\nGrab\n\n\n\n\n\nPass\n\n\n\n\n\nDrop"
  },
  {
    "objectID": "pages/projects/battery_estimator.html",
    "href": "pages/projects/battery_estimator.html",
    "title": "Battery SOC Estimator Interactive Web Simulation",
    "section": "",
    "text": "Estimating battery State of Charge (SOC) is used every day in our battery powered devices. However, it’s impractical to directly measure how much charge a battery has. One approach to this problem is the combination of a Luenberger Observer and a battery model that correlates Open Current Voltage (VOC) and State of Charge (SOC). The Luenberger Observer starts with a guess of the battery state of charge, and as it makes repeated measurements of VOC, it adjusts its SOC prediction until it converges to the true behavior measured by the battery estimator. This approach is effective if the behavior of the true battery is well understood, and if SOC can be observable by VOC.\nOne challenge with battery estimation is that sometimes the VOC-SOC curve has “plateaus” such that similar VOCs correspond drastically different SOCs, making the SOC effectively unobservable by VOC alone.\nThe effectiveness of the observer also depends on the accuracy of the model. As shown in the simulation, the estimator will not converge to the true SOC if the estimator’s model is different than the actual behavior (using the linear approximation vs true SOC curve).\nDifferent battery chemistries and physical properties have distinctive curves as show in Jeff Shepard’s article fantastic article.\nEnvironmental factors such as temperature and the formation of dendrites as the battery ages means the “true” model can evolve dynamically overtime, so maintaining an accurate SOC estimate requires the estimator model to also evolve accurately with the true behavior."
  },
  {
    "objectID": "pages/projects/battery_estimator.html#overview",
    "href": "pages/projects/battery_estimator.html#overview",
    "title": "Battery SOC Estimator Interactive Web Simulation",
    "section": "",
    "text": "Estimating battery State of Charge (SOC) is used every day in our battery powered devices. However, it’s impractical to directly measure how much charge a battery has. One approach to this problem is the combination of a Luenberger Observer and a battery model that correlates Open Current Voltage (VOC) and State of Charge (SOC). The Luenberger Observer starts with a guess of the battery state of charge, and as it makes repeated measurements of VOC, it adjusts its SOC prediction until it converges to the true behavior measured by the battery estimator. This approach is effective if the behavior of the true battery is well understood, and if SOC can be observable by VOC.\nOne challenge with battery estimation is that sometimes the VOC-SOC curve has “plateaus” such that similar VOCs correspond drastically different SOCs, making the SOC effectively unobservable by VOC alone.\nThe effectiveness of the observer also depends on the accuracy of the model. As shown in the simulation, the estimator will not converge to the true SOC if the estimator’s model is different than the actual behavior (using the linear approximation vs true SOC curve).\nDifferent battery chemistries and physical properties have distinctive curves as show in Jeff Shepard’s article fantastic article.\nEnvironmental factors such as temperature and the formation of dendrites as the battery ages means the “true” model can evolve dynamically overtime, so maintaining an accurate SOC estimate requires the estimator model to also evolve accurately with the true behavior."
  },
  {
    "objectID": "pages/projects/battery_estimator.html#simulation",
    "href": "pages/projects/battery_estimator.html#simulation",
    "title": "Battery SOC Estimator Interactive Web Simulation",
    "section": "Simulation",
    "text": "Simulation\n\n\n\nTrue Battery SOC and VOC Model\nThe battery is modeled as an RC pair with a series resistance of 0.05 Ω and a capacity of 7920 coulombs. These parameters correspond roughly to a typical 2200 mAh lithium-ion cell, which operates at approximately 3.6 V12. The initial state-of-charge (SOC) estimate is set to 50%.\nThe true battery voltage and the estimated voltage share the same functional form, but are computed using their respective SOC values. Gaussian measurement noise is applied to both voltage and current readings, with standard deviations of 100 mV and 10 mA, respectively. The SOC is updated over time by integrating the applied current divided by the battery capacity.\nTrue Voltage/SOC Curve:\n\\[\nV_\\text{OCV, nonlinear}(SOC) = 2.6 + 2.35 \\, SOC - 3.75 \\, SOC^2 + 2.5 \\, SOC^3\n\\]\nLinear Approximation of Cubic Model:\n\\[\nV_\\text{OCV, linear}(SOC) = 2.6 + 1.1 \\, SOC\n\\]\nBattery State-Space Model:\n\\[\n\\dot{SOC}(t) = \\frac{I(t)}{Q}, \\quad\nV_\\text{true}(t) = V_\\text{OCV}(SOC(t)) + R_s \\, I(t)\n\\]\n\n\\(k\\) : Discrete time step index\n\n\\(\\Delta t\\) : Time step (s)\n\n\n\n\nSOC Estimator\nThe SOC estimator follows a similar structure to the true model, with measurement noise added to the current to simulate sensor readings. A feedback term is applied, proportional to the difference between measured voltage and estimated voltage. This difference is scaled by the observer gain \\(K\\) (for the Luenberger observer) or by the Kalman gain (for the KF), which determines the corrective strength.\n\\[\n\\dot{\\hat{SOC}}(t) = \\frac{I(t) + n_I}{Q} + K \\left( V_\\text{true}(t) + n_V - \\hat{V}(t) \\right)\n\\]\n\\[\n\\hat{V}(t) = V_\\text{OCV}(\\hat{SOC}(t)) + R_s \\left(I(t) + n_I(t)\\right)\n\\]\n\n\\(\\hat{SOC}(t)\\) : Estimated SOC\n\n\\(\\hat{V}(t)\\) : Estimated voltage\n\n\\(n_I\\) : Current Noise\n\\(n_V\\) : Voltage Noise\n\\(K\\) : Observer gain"
  },
  {
    "objectID": "pages/projects/battery_estimator.html#results",
    "href": "pages/projects/battery_estimator.html#results",
    "title": "Battery SOC Estimator Interactive Web Simulation",
    "section": "Results",
    "text": "Results\nWhen the true VOC–SOC model is used for the estimator, the SOC estimate converges smoothly to the true SOC despite measurement noise. Quantitatively, under a 0.5 A discharge, the SOC error is approximately 4.7% at 8.3 minutes, while at a higher 2.2 A charge/discharge rate the error is about 4.4% at 500 seconds. After 500 seconds, the observer consistently estimates the true SOC within 5% under nominal conditions. In contrast, when the linear VOC–SOC approximation is used, the estimator remains close to the true SOC but fails to fully converge except at points where the linear and cubic models intersect. Adding noise does not alter this behavior since the difference between the models is not random.\nAdditionally, when using the true model, increasing the fixed observer gain reduces convergernce time to true SOC; however, excessively large gains introduce increased noise in the estimate. The Kalman filter converges to the true SOC significantly faster than the fixed-gain observer, but careful tuning of the process and measurement noise covariances is required to minimize estimate noise. Smaller initial SOC errors—particularly when starting closer to the true SOC in depleted battery conditions—further improve convergence behavior."
  },
  {
    "objectID": "pages/projects/battery_estimator.html#footnotes",
    "href": "pages/projects/battery_estimator.html#footnotes",
    "title": "Battery SOC Estimator Interactive Web Simulation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTenergy 18650 Li-ion Battery↩︎\nAdafruit 18650 Li-ion Battery↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Projects",
    "section": "",
    "text": "Documenting some of my favorite projects. More is on the way!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutonomous Maze Navigation with ROS2 and LiDAR\n\n\nA ROS2-based LiDAR navigation algorithm enabling autonomous maze escape on a TurtleBot3 platform.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBattery SOC Estimator Interactive Web Simulation\n\n\nExplore battery State-of-Charge estimation in your browser using a Fixed-Gain Observer or Kalman Filter.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCollaborative Lego Passing Robots\n\n\nProgramming two robots to autonomously transfer Legos, integrating vision-based detection, path planning, and real-time obstacle avoidance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHIFIMAN Headphone Joint Fix\n\n\nA 3D-printed structural splint addressing joint failures in early HIFIMAN HE-series headphones.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWake & Manipulate: Robot Arm\n\n\nRapid prototyping of a robotic arm, controller, and graphical user interface.\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/about.html",
    "href": "pages/about.html",
    "title": "Austin Lin",
    "section": "",
    "text": "University of Maryland, College Park | 2020 - 2024\nB.S. Mechanical Engineering\nMinor in Robotics and Autonomous Systems\nPublic Leadership Scholars Program"
  },
  {
    "objectID": "pages/about.html#education",
    "href": "pages/about.html#education",
    "title": "Austin Lin",
    "section": "",
    "text": "University of Maryland, College Park | 2020 - 2024\nB.S. Mechanical Engineering\nMinor in Robotics and Autonomous Systems\nPublic Leadership Scholars Program"
  },
  {
    "objectID": "pages/about.html#experience",
    "href": "pages/about.html#experience",
    "title": "Austin Lin",
    "section": "Experience",
    "text": "Experience\nGeneral Dynamics Electric Boat | Systems Engineer\nSeptember 2024 - Present\nL3Harris C5 | Systems Engineering Intern\nJune 2023 - August 2023\nUMD Space Systems Laboratory | Undergraduate Researcher\nMay 2022 - September 2022\n\n\nThis website was built with Quarto, and hosted on Github Pages."
  },
  {
    "objectID": "pages/projects/headphone_joint.html",
    "href": "pages/projects/headphone_joint.html",
    "title": "HIFIMAN Headphone Joint Fix",
    "section": "",
    "text": "HIFIMAN headphones have a reputation for delivering high audio quality at a discount price, but unfortunately, sometimes with discount build quality as well. Both joints attaching the driver to the headband failed on my pair of HE-400s just weeks after purchase, and this Head-Fi forum thread shows that these quality control issues are not uncommon across the early 2010s HIFIMAN headphone lineup.\n\n\n\nBroken headphone joint from the same Head-Fi forum thread)\n\n\nI decided to fix my broken headphones by designing a new enclosure that reinforces the preexisting hardware, and even after years of abuse, the joints have held together strong!\n\n\n\nFinal Headphone Joint Splint"
  },
  {
    "objectID": "pages/projects/headphone_joint.html#overview",
    "href": "pages/projects/headphone_joint.html#overview",
    "title": "HIFIMAN Headphone Joint Fix",
    "section": "",
    "text": "HIFIMAN headphones have a reputation for delivering high audio quality at a discount price, but unfortunately, sometimes with discount build quality as well. Both joints attaching the driver to the headband failed on my pair of HE-400s just weeks after purchase, and this Head-Fi forum thread shows that these quality control issues are not uncommon across the early 2010s HIFIMAN headphone lineup.\n\n\n\nBroken headphone joint from the same Head-Fi forum thread)\n\n\nI decided to fix my broken headphones by designing a new enclosure that reinforces the preexisting hardware, and even after years of abuse, the joints have held together strong!\n\n\n\nFinal Headphone Joint Splint"
  },
  {
    "objectID": "pages/projects/headphone_joint.html#files",
    "href": "pages/projects/headphone_joint.html#files",
    "title": "HIFIMAN Headphone Joint Fix",
    "section": "Files",
    "text": "Files\nI’ve attached the STL which can be printed as-is. Also attached is the Solidworks Model Part, so feel free to modify as you like!\nDownload jointfix_final.STL\nDownload jointfix_final.SLDPRT"
  },
  {
    "objectID": "pages/projects/headphone_joint.html#previous-iterations",
    "href": "pages/projects/headphone_joint.html#previous-iterations",
    "title": "HIFIMAN Headphone Joint Fix",
    "section": "Previous Iterations",
    "text": "Previous Iterations\n\nTape Fix\nI first tried using tape to hold the broken joint together, but it quickly became clear that it wasn’t going to work. Beyond the ✨aesthetics✨, the adhesive would weaken over time and the tape would begin to slide, eventually leaving the joint loosening. More importantly, the tape failed to address the underlying load path: instead of distributing the clamping force across the ear pads as intended, the headband concentrated that force into a small area at the end of the broken joint.\n\n\n\nTemporary Tape Fix\n\n\nI then attempted to repair the joint using epoxy. This approach introduced its own challenges, particularly keeping the epoxy from intruding into and damaging the sliding joint mechanism. The geometry of the failure also worked against this solution. The joint had sheared along a very small surface area, creating a localized high-stress region. Any small imperfections in the cured epoxy likely acted as stress concentrators, allowing cracks to slowly propagate until the joint failed again.\nAt that point, it became clear that simply restoring the original joint geometry was not sufficient. Rather than trying to recreate the failed interface, I shifted focus toward augmenting the joint by adding new structure carry the load more effectively in the form of a splint.\n\n\nProof of Concept\nMy initial concerns was function and sizing. Tolerancing had to be tight to prevent pieces from wiggling, and the splint had to be rigid enough to hold the parts together. Ergonomically, most of the pressure was along the top of the headband and earpads, but I also wanted to evaluate if adding thickness to then joint would impact comfort.\n\n\n\nVersion 1 and Tape Side by Side\n\n\n\n    \n\n\n\nPackaging\nThe initial splint held the two pieces tight and comfortably, but rocked back and forth. I modified its shape to conform better with the existing circular joint, and to make the overall splint smaller in size and more discrete.\n\n\n\nVersion 3\n\n\n\n    \n\n\n\nStrength and Finishing Features\nI noticed flexing in the joint/splint when I would stretch the drivers apart from eachother. Increasing the wall thickness and the radius of fillet reduced stress and increased rigidity drastically. Additional fillets and finishing touches were added to make the overall design more refined."
  },
  {
    "objectID": "pages/projects/robot_arm.html",
    "href": "pages/projects/robot_arm.html",
    "title": "Wake & Manipulate: Robot Arm",
    "section": "",
    "text": "The goal of this project is to demonstrate rapid prototyping of a small-scale robotic arm that can be controlled intuitively. The system automatically activates when a user is detected via an ultrasonic sensor and powers down when no user is present for convenience. An onboard OLED display provides real-time indication of whether the robot is active or inactive.\nDuring operation, the user can choose to control either the pitch motor or second yaw motor by pressing into the stick. A companion Processing-based visualization lists joint angles and the calculated end-effector position, allowing the user to monitor the robot’s configuration in real time."
  },
  {
    "objectID": "pages/projects/robot_arm.html#overview",
    "href": "pages/projects/robot_arm.html#overview",
    "title": "Wake & Manipulate: Robot Arm",
    "section": "",
    "text": "The goal of this project is to demonstrate rapid prototyping of a small-scale robotic arm that can be controlled intuitively. The system automatically activates when a user is detected via an ultrasonic sensor and powers down when no user is present for convenience. An onboard OLED display provides real-time indication of whether the robot is active or inactive.\nDuring operation, the user can choose to control either the pitch motor or second yaw motor by pressing into the stick. A companion Processing-based visualization lists joint angles and the calculated end-effector position, allowing the user to monitor the robot’s configuration in real time."
  },
  {
    "objectID": "pages/projects/robot_arm.html#architecture",
    "href": "pages/projects/robot_arm.html#architecture",
    "title": "Wake & Manipulate: Robot Arm",
    "section": "Architecture",
    "text": "Architecture\n\nRobot Arm: 3D-printed arm with multiple degrees of freedom. I initially tried to make the robot purely out of cardboard since this was the most widely available material I had the time, but later incorporated pencils to add more strength and rigidity to the structure. The robot arm has two yaw servo motors and a pitch servo motor, and connects directly to the controller. The next iteration would be to 3D print a enclosure for the electronics and the arm.\nController: The controller features an ultrasonic sensor, OLED display and a joystick. The breadboard is used to connect the arm, controller, and arduino together. A 6V battery pack is used as the power supply for the motors and arduino. Future improvements to the controller include designing a custom PCB and controller enclosure for much better packaging, as well as incorporating a physical switch to turn on/off the entire system, and relays to disconnect the motors from the power supply when the arm is turned off automatically.\nSoftware: The system software is divided between an Arduino-based controller and a Processing-based visualization that communicate over a serial connection. The Arduino handles sensing, control logic, and actuator commands, including user detection via an ultrasonic sensor and real-time joint control using a joystick with selectable control modes. An onboard OLED display provides immediate feedback on the system’s active state.\n\nJoint angles are transmitted to the host computer, where the Processing application parses the data, performs forward kinematics calculations, and renders a real-time visualization of joint angles and end-effector position. This separation allows reliable embedded control while providing clear, interactive feedback through the GUI."
  },
  {
    "objectID": "pages/projects/robot_arm.html#demo",
    "href": "pages/projects/robot_arm.html#demo",
    "title": "Wake & Manipulate: Robot Arm",
    "section": "Demo",
    "text": "Demo"
  },
  {
    "objectID": "pages/projects/robot_arm.html#visualization-code",
    "href": "pages/projects/robot_arm.html#visualization-code",
    "title": "Wake & Manipulate: Robot Arm",
    "section": "Visualization Code",
    "text": "Visualization Code\n\nInitialization and Setup\nThe environment is set up by defining basic arm parameters and opening a serial connection to the microcontroller. The serial buffer is cleared to ensure only valid runtime data is processed.\n\n\nShow code\n\nimport processing.serial.*; // add the serial library\nSerial myPort; // define a serial port object to monitor\nfloat armLength = 140; //length of arm in millimeters\nfloat[] armOrigin = {0, 0};\nfinal int armWidth = 10;\nfinal int windowWidth = 1000;\nfinal int windowLength = 1000;\n\nvoid setup() {\n\n  size(1000, 1300); // set the window size\n  println(Serial.list()); // list all available serial ports\n  myPort = new Serial(this, Serial.list()[1], 9600); // define input port\n  myPort.clear(); // clear the port of any initial junk\n  background(255);\n}\n\n\n\ndraw()\nThe draw() function handles real-time visualization of the robot arm by receiving joint angle data from the Arduino over serial communication. There are 3 main components:\n\nSerial Communication: The function first checks if data is available on the serial port, then reads a tab-delimited string containing three joint angles (theta1, pitch, theta2) and converts them from degrees to radians.\nForward Kinematics: Using the received angles and the predefined arm length, the function calculates the 2D positions of key points on the robot arm:\n\nProjects the arm length onto the 2D plane using the pitch angle\nComputes x and y coordinates for the midpoint of the first link, the midpoint of the second link, and the end-effector position\nUses trigonometric calculations based on the joint angles to determine each position relative to the origin\n\nArm Projection: This code projects the arm in 2D space. The code centers the arm on the screen and then draws the base joint. Then for each link, the coordinate frame is moved and rotated to draw the rectangles correctly. pushMatrix() and popMatrix() isolate transformations for each segment.\n\n\n\nShow code\n\nvoid draw () {\n\n  if (myPort.available () &lt;= 0) { //makes sure port is open\n    return;\n  }\n\n  //input processing\n  String inString = myPort.readStringUntil('\\n'); // read input string\n  if (inString != null) { // ignore null strings\n    background(255);\n    textSize(40);\n     text(\"Top View: \", 70, 70);\n    inString = trim(inString); // trim off any whitespace\n    String[] anglesInput = splitTokens(inString, \"\\t\"); // extract x & y into an array\n\n    float theta1 =  float(anglesInput[0]) * PI/180; //conversion to radians\n    float pitch =  float(anglesInput[1]) * PI/180;\n    float theta2 =  float(anglesInput[2]) * PI/180;\n\n    //math to find link/manipulator positions using angles and given lengths relative to an origin\n    //unit vector directions same as window\n    int length2d = int(armLength * cos(pitch));\n    int link1_x = int(length2d * cos(theta1) / 2);\n    int link2_x = link1_x*2 + int(length2d * cos(theta1 + theta2)/2);\n    int end_x = link1_x*2 + int(length2d * cos(theta1 + theta2));\n    int link1_y = int(length2d * sin(theta1) / 2);\n    int link2_y = link1_y*2 + int(length2d * sin(theta1 + theta2)/2);\n    int end_y = link1_y*2 + int( length2d * sin(theta1 + theta2));\n\n    //display angles\n    fill(0, 0, 0);    \n    \n    text(\"Yaw 1:\", 20, 930);\n    text(theta1*180/PI + \"°\", 140, 930);\n    fill(128,128,0);\n    \n    text(\"Yaw 2:\", 320, 930);\n    text(theta2*180/PI + \"°\", 440, 930);\n    fill(0,0,0);\n    \n    text(\"Pitch :\", 620, 930);\n    text(pitch*180/PI + \"°\", 740, 930);\n    \n    fill(0,128,128);\n    text(\"X:\", 20, 970);\n    text(\"Y:\", 320, 970);\n    text(end_x, 140, 970);\n    text(end_y, 440, 970);\n    fill(0,0,0);\n\n    //translates/rotates coordinate frame so that arm origin is at the center of the screen\n    pushMatrix();\n    translate(500, 600);\n    ellipse(0,0,10,10);\n    pushMatrix(); //save origin frame\n    \n    //link 1\n    pushMatrix(); //creates copy of origin frame for link 1\n    rectMode(CENTER); //sets coordinate args to center of rectangle\n    translate(link1_x, -link1_y);\n    rotate(-theta1);\n    rect(0, 0, length2d, armWidth);\n    popMatrix();\n\n    //link 2\n    pushMatrix();\n    translate(link2_x, -link2_y);\n    rotate(-1*(theta1 + theta2));\n    fill(128,128,0);\n    rect(0, 0, length2d, armWidth);// draw rectangle\n    popMatrix();\n    //end effector\n    translate(end_x, -end_y);\n      fill(0,128,128);\n    ellipse(0,0,10,10);\n\n    //clear stack\n    popMatrix();\n    popMatrix();\n    fill(0,0,0);\n\n  }\n}"
  },
  {
    "objectID": "pages/projects/robot_arm.html#controller-code",
    "href": "pages/projects/robot_arm.html#controller-code",
    "title": "Wake & Manipulate: Robot Arm",
    "section": "Controller Code",
    "text": "Controller Code\n\nInitialization and Setup\nPins used for the motor and controller are defined on the arduino. Joystick deadzone and debounce, and ultrasonic sensing distance are also defined. The “on” and “off” image are also defined for the OLED display as a bitmap array.\n\n\nShow code\n\n#include &lt;Wire.h&gt;\n#include &lt;Adafruit_GFX.h&gt;\n#include &lt;Adafruit_SSD1306.h&gt;\n \n//oled reset\n#define OLED_RESET 4\n \n//servo pins\n#define yaw1 10\n#define pitch 11\n#define yaw2 12\n \n//sensor pins\n#define triggerPin 7\n#define echoPin 6\n#define xPin 1\n#define yPin A0\n#define switchPin 2\n#define motorSpeed 2\n \n//deadzone and debounce for joystick\n#define tolerance 200\n#define debounce 150\n \nAdafruit_SSD1306 display(128, 64, &Wire, OLED_RESET);\n \nint startTime, currentTime;  // for timing loop\nlong duration;\nint distance;\n//angles range from 0 - 180\nfloat theta1 = 0;\nfloat theta2 = 0;\nfloat theta3 = 0;\nString dispImg = \"\";\nbool pitchMode = false;\n\n\n\nShow OLED Bitmap\n\n//bitmaps generated from https://diyusthad.com/image2cpp\n// 'OFF, 128x64px\nconst unsigned char offImg[] PROGMEM = {\n \n  0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n  0x3f, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xfc,\n  0x7f, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xfe,\n  0x7f, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xfe,\n  0x70, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x00, 0x03, 0xff, 0x80, 0x00, 0x7f, 0xff, 0xfe, 0x0f, 0xff, 0xff, 0xe0, 0x00, 0x0e,\n  0x70, 0x00, 0x00, 0x1f, 0xff, 0xf0, 0x00, 0x7f, 0xff, 0xfe, 0x0f, 0xff, 0xff, 0xe0, 0x00, 0x0e,\n  0x70, 0x00, 0x00, 0x3f, 0xff, 0xf8, 0x00, 0x7f, 0xff, 0xfe, 0x0f, 0xff, 0xff, 0xe0, 0x00, 0x0e,\n  0x70, 0x00, 0x00, 0xff, 0xff, 0xfc, 0x00, 0x7f, 0xff, 0xfe, 0x0f, 0xff, 0xff, 0xe0, 0x00, 0x0e,\n  0x70, 0x00, 0x01, 0xff, 0xff, 0xfe, 0x00, 0x7f, 0xff, 0xfe, 0x0f, 0xff, 0xff, 0xe0, 0x00, 0x0e,\n  0x70, 0x00, 0x03, 0xff, 0x01, 0xff, 0x00, 0x7e, 0x00, 0x00, 0x0f, 0xe0, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x03, 0xfc, 0x00, 0x7f, 0x80, 0x7e, 0x00, 0x00, 0x0f, 0xe0, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x07, 0xf8, 0x00, 0x3f, 0xc0, 0x7e, 0x00, 0x00, 0x0f, 0xe0, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x07, 0xf0, 0x00, 0x1f, 0xc0, 0x7e, 0x00, 0x00, 0x0f, 0xe0, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x0f, 0xe0, 0x00, 0x1f, 0xc0, 0x7e, 0x00, 0x00, 0x0f, 0xe0, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x0f, 0xe0, 0x00, 0x0f, 0xe0, 0x7e, 0x00, 0x00, 0x0f, 0xe0, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x0f, 0xe0, 0x00, 0x0f, 0xe0, 0x7e, 0x00, 0x00, 0x0f, 0xe0, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x0f, 0xe0, 0x00, 0x0f, 0xe0, 0x7f, 0xff, 0xf8, 0x0f, 0xff, 0xff, 0x80, 0x00, 0x0e,\n  0x70, 0x00, 0x0f, 0xc0, 0x00, 0x0f, 0xe0, 0x7f, 0xff, 0xf8, 0x0f, 0xff, 0xff, 0x80, 0x00, 0x0e,\n  0x70, 0x00, 0x0f, 0xc0, 0x00, 0x0f, 0xe0, 0x7f, 0xff, 0xf8, 0x0f, 0xff, 0xff, 0x80, 0x00, 0x0e,\n  0x70, 0x00, 0x0f, 0xc0, 0x00, 0x0f, 0xe0, 0x7f, 0xff, 0xf8, 0x0f, 0xff, 0xff, 0x80, 0x00, 0x0e,\n  0x70, 0x00, 0x0f, 0xc0, 0x00, 0x0f, 0xe0, 0x7f, 0xff, 0xf8, 0x0f, 0xff, 0xff, 0x80, 0x00, 0x0e,\n  0x70, 0x00, 0x0f, 0xe0, 0x00, 0x0f, 0xe0, 0x7e, 0x00, 0x00, 0x0f, 0xe0, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x0f, 0xe0, 0x00, 0x0f, 0xe0, 0x7e, 0x00, 0x00, 0x0f, 0xe0, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x0f, 0xe0, 0x00, 0x0f, 0xe0, 0x7e, 0x00, 0x00, 0x0f, 0xe0, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x0f, 0xf0, 0x00, 0x1f, 0xc0, 0x7e, 0x00, 0x00, 0x0f, 0xe0, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x07, 0xf0, 0x00, 0x1f, 0xc0, 0x7e, 0x00, 0x00, 0x0f, 0xe0, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x07, 0xf8, 0x00, 0x3f, 0xc0, 0x7e, 0x00, 0x00, 0x0f, 0xe0, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x03, 0xfc, 0x00, 0x7f, 0x80, 0x7e, 0x00, 0x00, 0x0f, 0xe0, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x03, 0xff, 0x03, 0xff, 0x00, 0x7e, 0x00, 0x00, 0x0f, 0xe0, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x01, 0xff, 0xff, 0xfe, 0x00, 0x7e, 0x00, 0x00, 0x0f, 0xe0, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x00, 0xff, 0xff, 0xfc, 0x00, 0x7e, 0x00, 0x00, 0x0f, 0xe0, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x00, 0x3f, 0xff, 0xf8, 0x00, 0x7e, 0x00, 0x00, 0x0f, 0xe0, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x00, 0x0f, 0xff, 0xe0, 0x00, 0x7e, 0x00, 0x00, 0x0f, 0xe0, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x00, 0x01, 0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e,\n  0x70, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e,\n  0x78, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e,\n  0x7f, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xfe,\n  0x3f, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xfe,\n  0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00\n \n};\n\n// 'ON', 128x64px\nconst unsigned char onImg[] PROGMEM = {\n  0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff,\n  0xff, 0x80, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x01, 0xff,\n  0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xff,\n  0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xff, 0xe0, 0x0f, 0xff, 0xe0, 0xff, 0xf0, 0x7f, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xff, 0x00, 0x01, 0xff, 0xc0, 0x7f, 0xf0, 0x7f, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xfe, 0x00, 0x00, 0xff, 0xc0, 0x7f, 0xf0, 0x7f, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xfc, 0x00, 0x00, 0x7f, 0xc0, 0x3f, 0xf0, 0x7f, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xf8, 0x00, 0x00, 0x3f, 0xc0, 0x1f, 0xf0, 0x7f, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xf0, 0x07, 0xe0, 0x1f, 0xc0, 0x1f, 0xf0, 0x7f, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xf0, 0x1f, 0xf0, 0x0f, 0xc0, 0x0f, 0xf0, 0x7f, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xe0, 0x3f, 0xf8, 0x0f, 0xc0, 0x0f, 0xf0, 0x7f, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xe0, 0x7f, 0xfc, 0x07, 0xc0, 0x07, 0xf0, 0x7f, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xc0, 0x7f, 0xfe, 0x07, 0xc0, 0x03, 0xf0, 0x7f, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xc0, 0xff, 0xfe, 0x07, 0xc0, 0x03, 0xf0, 0x7f, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xc0, 0xff, 0xfe, 0x07, 0xc0, 0x01, 0xf0, 0x7f, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xc0, 0xff, 0xff, 0x03, 0xc0, 0x81, 0xf0, 0x7f, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xc0, 0xff, 0xff, 0x03, 0xc0, 0xc0, 0xf0, 0x7f, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xc0, 0xff, 0xff, 0x03, 0xc0, 0xc0, 0x70, 0x7f, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xc0, 0xff, 0xff, 0x03, 0xc0, 0xe0, 0x70, 0x7f, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xc0, 0xff, 0xff, 0x03, 0xc0, 0xe0, 0x30, 0x7f, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xc0, 0xff, 0xfe, 0x03, 0xc0, 0xf0, 0x10, 0x7f, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xc0, 0xff, 0xfe, 0x07, 0xc0, 0xf8, 0x10, 0x7f, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xc0, 0xff, 0xfe, 0x07, 0xc0, 0xf8, 0x00, 0x7f, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xc0, 0x7f, 0xfe, 0x07, 0xc0, 0xfc, 0x00, 0x7f, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xe0, 0x7f, 0xfc, 0x07, 0xc0, 0xfe, 0x00, 0x7f, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xe0, 0x3f, 0xf8, 0x0f, 0xc0, 0xfe, 0x00, 0x7f, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xe0, 0x1f, 0xf0, 0x0f, 0xc0, 0xff, 0x00, 0x7f, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xf0, 0x07, 0xe0, 0x1f, 0xc0, 0xff, 0x00, 0x7f, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xf8, 0x00, 0x00, 0x3f, 0xc0, 0xff, 0x80, 0x7f, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xfc, 0x00, 0x00, 0x7f, 0xc0, 0xff, 0xc0, 0x7f, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xfe, 0x00, 0x00, 0xff, 0xc0, 0xff, 0xc0, 0x7f, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xff, 0x80, 0x01, 0xff, 0xc0, 0xff, 0xe0, 0x7f, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xff, 0xe0, 0x0f, 0xff, 0xe0, 0xff, 0xf0, 0x7f, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x1f, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xf8, 0xff,\n  0xff, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xff,\n  0xff, 0x80, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xff,\n  0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff\n};\n\n\n\nShow code\n\nvoid setup() {\n  //setup timing loop, serial, and pin\n  startTime = millis();\n \n  pinMode(yaw1, OUTPUT);  //setup digital pin\n  pinMode(yaw2, OUTPUT);\n  pinMode(pitch, OUTPUT);\n  pinMode(switchPin, INPUT_PULLUP);\n  pinMode(triggerPin, OUTPUT);  // Sets the trigPin as an Output\n  pinMode(echoPin, INPUT);      // Sets the echoPin as an Input\n  clearDisplay();\n  Serial.begin(9600);\n \n \n  //resets servos to initial position\n  servoControl(0, pitch);\n  servoControl(0, yaw1);\n  servoControl(0, yaw2);\n}\n\n\n\nMain Loop\nThe microcontroller performs 4 main actions in the loop:\n\nAutomatic Activation: The ultrasonic sensor checks if a user is within 60cm. If no user is detected, the system displays “OFF” on the OLED and exits the loop with no motor commands sent, effectively deactivating motor control. When a user approaches, it displays “ON” and proceeds with normal operation.\nJoystick Control: The joystick’s X-axis always controls the first yaw joint (theta1). The Y-axis control is mode-dependent: in pitch mode, it controls the vertical pitch joint (theta2); otherwise, it controls the second yaw joint (theta3). All joint movements are constrained within 0-180 degrees due to the servo hardware.\nMode Switching: Pressing the joystick button toggles between pitch and yaw control modes for the Y-axis.\nData Transmission: At the end of each loop cycle, all three joint angles are sent via serial communication to the Processing visualization as tab-separated values.\n\n\n\nShow code\n\nvoid loop() {\n \n  //ultrasonic activation\n  if (ultraSonic() &gt; 60) {\n \n    //clears oled display if needed\n    if (dispImg == \"on\") {\n      clearDisplay();\n    }\n    dispImg = \"off\";\n    drawOff();\n    return;\n  } else {\n    //clears oled display if needed\n    if (dispImg == \"off\") {\n      clearDisplay();\n    }\n    drawOn();\n    dispImg = \"on\";\n  }\n \n  //joystick inputs update theta\n  int dX = readX();\n \n  if (dX + theta1 &lt;= 180 && dX + theta1 &gt;= 0) {  //range bounds\n    theta1 += dX;\n    servoControl(theta1, yaw1);\n  }\n  int dY = readY();\n  if (dY + theta2 &lt;= 180 && dY + theta2 &gt;= 0 && pitchMode) {  //range bounds\n    theta2 += dY;\n    servoControl(theta2, pitch);\n  }\n  if (dY + theta3 &lt;= 180 && dY + theta3 &gt;= 0 && !pitchMode) {  //range bounds\n    theta3 += dY;\n    servoControl(theta3, yaw2);\n  }\n\n  //toggle between modes\n  if (analogRead(switchPin) &lt;= 5) {\n    pitchMode = !pitchMode;\n    delay(debounce);  //debounce\n  }\n \n \n  //angles for processing\n  Serial.println(String(theta1) + \"\\t\" + String(theta2) + \"\\t\" + String(theta3));\n}\n\n\n\nHelper Functions\n\nJoystick Reading (readAxis, readX, readY): Reads analog joystick input and converts it into directional commands. The function implements a deadzone (tolerance of 200 around the center value of 507) to ignore small unintentional movements, returning the motor speed multiplied by direction (-1 for left/down, 0 for center, +1 for right/up).\nServo Control (pwmPulse, servoControl): Generates PWM signals to control the servos without using a servo library. The servoControl function maps desired angles (0-330°) to pulse widths (550-2450 microseconds) and ensures a 20ms period between pulses, which is the standard servo control frequency.\nUltrasonic Sensing (ultraSonic): Measures distance using an ultrasonic sensor by sending a trigger pulse, measuring the echo return time, and converting it to distance in centimeters using the speed of sound at sea level (0.034 cm/microsecond divided by 2 for round-trip).\nDisplay Functions (drawOn, drawOff, clearDisplay): Render the predefined “ON” and “OFF” bitmaps to the OLED display and handle display initialization and clearing.\n\n\n\nShow code\n\nint readAxis(int pin) {  //returns 1 if right, zero if centered, -1 if left\n  int reading = analogRead(pin);\n  if (reading &lt; 507 - tolerance) {\n    return -1 * motorSpeed;\n  }\n  if (reading &gt; 507 + tolerance) {\n    return motorSpeed;\n  }\n  return 0;\n}\n \nint readX() {\n  return readAxis(xPin);\n}\nint readY() {\n  return readAxis(yPin);\n}\n \n//PWM pulse\nvoid pwmPulse(int period, int pin) {\n  digitalWrite(pin, LOW);\n  delayMicroseconds(10);\n  digitalWrite(pin, HIGH);    // turn the LED on (HIGH is the voltage level)\n  delayMicroseconds(period);  // wait for delayLength\n  digitalWrite(pin, LOW);     // turn the LED off by making the voltage LOW\n}\n \n//input- servo angle\nvoid servoControl(float angle, int pin) {\n  int tControl = map(angle, 0, 330, 550, 2450);\n \n  //timing loop for PWM 20ms period\n  currentTime = millis();\n  if ((currentTime - startTime) &gt;= 20) {\n    pwmPulse(tControl, pin);  //pulse\n    startTime = currentTime;\n  }\n  delay(20);\n}\n \nint ultraSonic() {\n  // reset trigger\n  pwmPulse(10, triggerPin);\n  duration = pulseIn(echoPin, HIGH);  //listening for echo\n  distance = duration * 0.034 / 2;    // distance conversion\n  return distance;\n}\n \n//draws \"on\" on oled display\nvoid drawOn() {\n  display.drawBitmap(0, 0, onImg, 128, 64, WHITE);  // display.drawBitmap(x position, y position, bitmap data, bitmap width, bitmap height, color)\n  display.display();\n}\n \nvoid drawOff() {\n  display.drawBitmap(0, 0, offImg, 128, 64, WHITE);  // display.drawBitmap(x position, y position, bitmap data, bitmap width, bitmap height, color)\n  display.display();\n}\n \nvoid clearDisplay() {\n  display.begin(SSD1306_SWITCHCAPVCC, 0x3C);\n  display.clearDisplay();\n}"
  }
]